---
title: "Multivariate"
format: html
---



# Setup  
```{r}
library(tidyverse)
library(ggcorrplot)
library(broom)
library(car)
library(factoextra)
library(ggpmisc)
```

```{r}
samples <- read_csv("../data/Samples.csv")

samples
```


# EDA  
How are variables related? Any strong correlations that we should watch out for?  

Let's create a correlation matrix
```{r correlation matrix p-values}
# Estimating significance matrix
p.mat <- samples %>%
  dplyr::select(-c(1:3)) %>%
  cor_pmat()

p.mat
```

Plotting  
```{r correlation matrix plot}
samples %>%
  dplyr::select(-c(1:3)) %>%
  cor() %>%
  ggcorrplot(hc.order = TRUE, 
             digits = 1,
             type = "lower", 
             p.mat = p.mat, 
             sig.level = 0.05,
             insig = "blank",
             lab = TRUE)

ggsave("../output/corrmat.png",
       height = 33,
       width = 30,
       bg = "white")
```

A bit difficult to see because we have so many predictors, let's extract in table format.  

```{r highest correlations}
samples %>%
  dplyr::select(-c(1:3)) %>%
  cor() %>% # calculates r (-1 to 1)
  as.data.frame() %>%
  rownames_to_column() %>%
  pivot_longer(cols = -rowname) %>%
  filter(abs(value) > .85 & value != 1) %>%
  arrange(desc(value)) %>%
  distinct(value, .keep_all = T)

```

```{r lowest correlations}
samples %>%
  dplyr::select(-c(1:3)) %>%
  cor() %>% # calculates r (-1 to 1)
  as.data.frame() %>%
  rownames_to_column() %>%
  pivot_longer(cols = -rowname) %>%
  filter(abs(value) < .05 & value != 1) %>%
  arrange(abs(value)) %>%
  distinct(value, .keep_all = T)

```

How do variables relate to fiber strength in a bivariate relationship?  

```{r r2}
samples %>%
  dplyr::select(-c(1:3)) %>%
  pivot_longer(cols=-class) %>%
  group_by(name) %>%
  nest() %>%
  mutate(r2 = map_dbl(data,
                  ~lm(class ~ value,
                      data = .x) %>%
                    glance(.) %>%
                    pull(r.squared)
                  )) %>%
  arrange(desc(r2))


```

```{r r2 plot}
samples %>%
  dplyr::select(-c(1:3)) %>%
  pivot_longer(cols=-class) %>%
  group_by(name) %>%
  nest() %>%
  mutate(r2 = map_dbl(data,
                  ~lm(class ~ value,
                      data = .x) %>%
                    glance(.) %>%
                    pull(r.squared)
                  )) %>%
  arrange(desc(r2)) %>%
  ungroup() %>%
  slice(1:6) %>% 
  unnest(data) %>%
  ggplot(aes(x = value, 
             y = class))+
  geom_point(shape = 21, 
             alpha = .7, 
             fill = "purple")+
  geom_smooth(method = "lm", 
              se = F, 
              color = "black", 
              linewidth = 1)+
  facet_wrap(~name, 
             scales = "free_x", 
             ncol=2) 

```

# Multicollinearity 
## Concepts  
Multicollinearity definition: more than two explanatory variables in a multiple regression model are highly linearly related.  


Multicollinearity is an issue because:  

- Model estimates magnitude and direction (+ or -) can change for multicollinear variables compared to a non-multicollinear model.  
  
- Model estimates standard error are inflated, directly affecting p-values, estimate significance, and power.  
  

## Applied example  
Let's select a few variables to run some tests.  
Two uncorrelated variables:
```{r uncorrelated}
#weather %>%
  #ggplot(aes(x = mean_dayl.s_Apr, 
             #y = sum_prcp.mm_Oct)) +
  #geom_point() +
  #geom_smooth(method="lm") +
  #stat_correlation()
  
# Calculate the correlation coefficient
cor_coef <- cor(weather$mean_dayl.s_Apr, weather$sum_prcp.mm_Oct)

# Create the plot with manual annotation
weather %>%
  ggplot(aes(x = mean_dayl.s_Apr, y = sum_prcp.mm_Oct)) +
  geom_point() +
  geom_smooth(method = "lm") +
  annotate("text", x = Inf, y = Inf, label = paste("Correlation:", round(cor_coef, 2)), 
           hjust = 1.1, vjust = 1.1, size = 5)

```

Two correlated variables:
```{r correlated}
weather %>%
  ggplot(aes(x = mean_dayl.s_Apr, 
             y = mean_dayl.s_May))+
  geom_point() +
  geom_smooth(method="lm") + 
  stat_correlation()

```

Now let's fit some models with one or two uncorrelated and correlated variables explaining fiberstrength and see what happens.  

```{r strength ~ mean_dayl.s_Apr}
lm_dayl.apr <- lm(strength_gtex ~ mean_dayl.s_Apr,
                  data = weather) %>%
  tidy() %>%
  filter(term != "(Intercept)") %>%
  mutate(mode = "lm_dayl.apr")

lm_dayl.apr

```

```{r strength ~ sum_prcp.mm_Oct}
lm_prp.oct <- lm(strength_gtex ~ sum_prcp.mm_Oct,
   data = weather) %>%
  tidy() %>%
  filter(term!="(Intercept)") %>%
  mutate(mod="lm_prp.oct")

lm_prp.oct
```

```{r strength ~ mean_dayl.s_Apr + sum_prcp.mm_Oct}
lm_dayl.prcp <- lm(strength_gtex ~ mean_dayl.s_Apr + 
                   sum_prcp.mm_Oct,
   data = weather) %>%
  tidy() %>%
  filter(term != "(Intercept)") %>%
  mutate(mod = "lm_2var.uncor")

lm_dayl.prcp

```

```{r strength ~ mean_dayl.s_Apr + mean_dayl.s_May}
lm_daylapr.daylmay <- lm(strength_gtex ~ mean_dayl.s_Apr + 
                  mean_dayl.s_May,
   data = weather) %>%
  tidy() %>%
  filter(term!="(Intercept)") %>%
  mutate(mod="lm_2var.cor")

lm_daylapr.daylmay

```

```{r Checking multicollinearity}
lm_dayl.apr %>%
  bind_rows(lm_prp.oct, 
            lm_dayl.prcp,
            lm_daylapr.daylmay) %>%
  mutate(mod=factor(mod,
                    levels=c("lm_dayl.apr",
                             "lm_prp.oct",
                             "lm_2var.uncor",
                             "lm_2var.cor"))) %>%
  filter(term != "mean_dayl.s_May") %>%
  ggplot(aes(x=mod)) +
  geom_pointrange(aes(y = estimate,
                      ymin = estimate - std.error,
                      ymax = estimate + std.error
                      )) +
  facet_wrap(~ term, scales = "free_y") +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))

```

What has happened with **mean_dayl.s_April** and **sum_prcp.mm_Oct** estimates and standard error when modeled i) alone, or with another uncorrelated variable vs. ii) with another correlated variable?

One way to check the degree of multicollinearity between two predictos is with the variance inflation factor (VIF).  

VIF values range from 1 to positive infinite.  
General rule of thumb:  

- VIF ~ 1: no multicollinearity  
- VIF between 1 and 5: moderate multicollinearity  
- VIF > 5: severe multicollinearity  

Let's check the variance inflation factor (VIF) of both uncorrelated and correlated models
```{r vif uncorrelated}
# Uncorrelated
lm(strength_gtex ~ mean_dayl.s_Apr + 
                   sum_prcp.mm_Oct,
   data = weather) %>%
  vif()
```


```{r vif correlated}
# Correlated
lm(strength_gtex ~ mean_dayl.s_Apr + 
                  mean_dayl.s_May,
   data = weather) %>%
  vif()

```

## Dealing with multicollinearity  
So what now?  
How can we deal with correlated variables in a multivariate approach?  
Options:  

- Dimensionality reduction  
    - By hand (selecting predictors based on previous knowledge, literature, etc.)  
    - By math  
  
- Algorithm that handles multicollinearity  
    - Variable selection    
    - Multivariate by ensembling multiple bivariates   

Many multivariate approaches deal with some sort of similarity/dissimilarity measure among predictors.  

In those cases, predictors with vastly different scales (e.g. mean_tmin.c_Jan from -10 to 10 C vs mean_dayl.s_Jul from 48000 to 51000) need to be normalized so measurement scale does not affect variable importance.    

Thus, our numerical predictor variables need to be normalized (center and scale) before starting our multivariate analysis.  

Some analysis do the normalization for you (like PCA), and others don't (like k-means), so need to be aware of this to ensure data is normalized.  

Since both PCA and k-means only take numerical variables, let's select them now.  
```{r selecting only numerical vars}
samples_n <- samples %>%
  dplyr::select((-c(1:3)),-44)

samples_n
```

# PCA  
Principal component analysis (PCA) is a dimensionality reduction approach that accomodates only numerical variables.  

Finds linear relationships among predictors that can be represented in a lower number of uncorrelated dimensions.  

Works well when at least some predictors are correlated.  

PCA:  

- Is used for dimensionality reduction  
- Is an unsupervised analysis (no outcome)  
- Only takes predictors  
- Predictors need to be numerical  

Some PCA properties:  

- The number of PCs calculated is the same as the number of variables in the data set (e.g., in our case, 72).  

- The overall variance explained by each PC is greatest for PC1 and decreases as PC number increases (e.g., PC72 will explain the least variance).  

- All PCs are **orthogonal** (i.e., independent of each other).  

- The goal is to select a small number of PCs that explain a minimum threshold of the total variance (e.g., 70%).  

```{r pca in action}
knitr::include_graphics("https://miro.medium.com/v2/resize:fit:1200/1*V4H3Cu8qGr_90WANKSO9BA.gif")
```
Let's run PCA for our weather data.  

```{r pca model}
mod_pca <- prcomp(samples_n, scale. = T)
mod_pca

summary(mod_pca)
```

## Choosing number of PCs  
Based on scree plot (total variance):  
```{r pca checking number of components}
# Scree plot
fviz_eig(mod_pca,
         
         addlabels = T)
```
    
PCs 1 and 2 explain ~58.7% and ~14.1% (72.8%) of total variance.
This indicates that original variables were correlated (as we expected).   

If wanted to use enough PCs to explain 70% of total variance, how many would we need?  
```{r PCs to explain 70pct variance}
mod_pca %>%
  get_eig() %>%
  mutate(pc = 1:nrow(.)) %>%
  ggplot(aes(x = pc,
         y = cumulative.variance.percent)) +
  geom_bar(stat = "identity") +
  geom_hline(yintercept = 70)

```
    
We would need 2 PCs.  

## Inspecting PCs  
Let's inspect PC1 through the loadings (weights) of each variable towards it.  

What are the weights that each variable received in this PC?  
```{r PC1 weights}
mod_pca$rotation %>%
  as.data.frame() %>%
  rownames_to_column(var = "var") %>%
  ggplot(aes(x = reorder(var,desc(PC1)), 
             y = PC1))+
  geom_bar(stat = "identity", 
           aes(fill = PC1), 
           show.legend = F)+
  scale_fill_gradient(low = "red", high = "blue")+
  theme(axis.text.x = element_text(angle = 45, 
                                   hjust = 1))

```

Which variables contributed most to PC1, regardless of direction? 

B05n

```{r variables contributing to PC1}
fviz_contrib(mod_pca,
             choice = "var",
             axes = 1
             )
```
  (red line is averall average contribution)
Let's check the eigenvectors for both PCs 1 and 2 variables:  

```{r pca variable contribution }
fviz_pca_var(mod_pca, 
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE,     # Avoid text overlapping
             select.var = list(contrib = 5)
             )

```
  
The longer is the eigenvector for a given variable, the more important it is towards that PC.  

Let's plot PC1 vs PC2 scores, look for any groupings.  

```{r PC1 vs PC2}
fviz_pca_ind(mod_pca,
             label = F)
```
Some grouping might exist
What did we learn?  

- Some original variables strongly correlated  
- Need at least 4 PCs to cover ~70% of original variables variance  
- Most important variables in PC1 were related to day length in winter and summer months.  

What now?  
Let's add the first 4 PCs to our original dataset and run a regression versus grain yield.    

```{r pca scores}
# Extract first 4 PCs scores
pca_scores <- mod_pca$x %>%
  as.data.frame() %>%
  dplyr::select(PC1:PC2)

pca_scores
```

```{r pca regression}
# Adding PCs 1-4 scores to original data set
samples_postpca <- samples %>%
  bind_cols(pca_scores)

# Regression of yield ~ PCs

lm_pca <- lm(class ~ PC1 + PC2,
             data = vi_postpca)

# Summary  
summary(lm_pca)
```
PC1 have a significant effect in strength

```{r pca regression}
# Checking VIF (variance inflation vactor)
vif(lm_pca)
```

```{r pca regression plots}
# Plotting strength vs PC1
ggplot(vi_postpca,
       aes(x = PC1,
           y = class)) +
  geom_point() +
  geom_smooth(method = "lm")
```


```{r pca regression plots}
# Plotting strength vs PC2
ggplot(vi_postpca, 
       aes(x = PC2, y = class))+
  geom_point()+
  geom_smooth(method = "lm")

```


```{r pca regression plots}
# Plotting strength vs PC3
ggplot(vi_postpca, 
       aes(x = PC3, y = class))+
  geom_point()+
  geom_smooth(method = "lm")

```

Only 1 explained class (look at which variables were most important to each PC for interpretation).  

As expected, PCs were not multicollinear (VIF=1).  

# k-means  
k-means is an **unsupervised** clustering algorithm and partitions the data into k groups, where k is defined by the user.  

k-means works by  

- randomly choosing k samples from our data to be the initial cluster centers, 
- calculates the distance of all observations to the clusters centers, 
- assigns a cluster class to each observation based on closest distance
- using all members of a cluster, recalculates cluster mean
- repeats the entire process until cluster means stabilize
```{r k-means in action}
knitr::include_graphics("https://miro.medium.com/max/960/1*KrcZK0xYgTa4qFrVr0fO2w.gif")

```

k-means:  

- Is used for clustering  
- Is an unsupervised analysis (no outcome)  
- Only takes predictors  
- Predictors need to be numerical  
- Does not handle NAs  


k-means is useful when clusters are circular, but can fail badly when clusters have odd shapes or outliers.  

```{r clustering algorithms comparison}
knitr::include_graphics("https://miro.medium.com/max/1400/1*oNt9G9UpVhtyFLDBwEMf8Q.png")

```

k-means does not normalize our data for us like PCA did, so we will need to do that before running the model.  

```{r}
# normalizing the data
vi_norm <- vi_n %>%
  mutate(across(everything(), ~scale(.x)))

vi_norm

summary(vi_norm)
```

Also, we need to define the number of clusters we want.  
Any thoughts?  
Let's try 2.  

```{r kmeans model }
mod_km <- kmeans(vi_norm,
                 centers = 6,
                 nstart = 10)

mod_km
```

Since the choice of k can be subjective, we will need to find an objective way to select the value of k that most properly represents our dataset.  

```{r choosing k - total error}
# Total error x k
fviz_nbclust(vi_norm,
             method = "wss",
             k.max = 10,
             FUNcluster = kmeans)
```


```{r choosing k - silhouette}
# Silhouette width
fviz_nbclust(vi_norm, 
             method = "s",
             k.max = 10,
             FUNcluster = kmeans) 

```

total error: k=3-4  
silhouette: k=3  

Let's go with 4 clusters.  

```{r mod_km4 }
mod_km4 <- kmeans(vi_norm,
                  centers = 3,
                  nstart = 10)

mod_km4
```

How many observations per cluster?
```{r}
vi %>%
  mutate(cluster = mod_km4$cluster) %>%
  group_by(cluster) %>%
  tally()

```

Now how can we visually inspect the resutls of k-means?  
We can either  

- add the cluster column to original dataset and explore the distribution of each variable against cluster id, OR  

- use a function that summarises all the original variables into PCs and plots the cluster ids.  

```{r cluster x variable boxplots}
vi %>%
  mutate(cluster = mod_km4$cluster,
         cluster = factor(cluster)) %>%
  pivot_longer(!c(x, y, cluster)) %>%
  ggplot(aes(x = cluster, 
             y = value, 
             color = cluster))+
    geom_boxplot(show.legend = F)+
  facet_wrap(~name, scales = "free_y", ncol = 6)

ggsave("../output/clustervalidation.png",
       width = 10,
       height = 20)  

```
  
We could actually run ANOVA models for each original variable of the form  

              var ~ cluster, 
              for ex. mean_dayl.s_Jan ~ cluster  
  
and extract cluster mean and pairwise comparison to understand what variables had significant differences among clusters.  

```{r kmeans PCA plot}
fviz_cluster(mod_km4,
             data = vi_norm)
```
  
Notice how, behind the scenes, the fviz_cluster function ran a PCA and is showing us a plot with PCs 1 and 2 on the axis (same result as we obtained on the PCA analysis).   


# Summary  
In this exercise, we covered:  

  - When multivariate analysis can be used  
  - How multicollinearity is an issue and what to do to fix it   
  - PCA for dimensionality reduction  
  - k-means for clustering  
  - How to validate results from both analysis  




  
